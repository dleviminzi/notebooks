{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "torch",
   "display_name": "torch",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Requirements"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from pathlib import Path"
   ]
  },
  {
   "source": [
    "## Train Tokenizer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = ['../raw_data/train.csv', '../raw_data/val.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertWordPieceTokenizer(clean_text=True, strip_accents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['steam_tokenized/vocab.txt']"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, show_progress=True)\n",
    "tokenizer.save_model(\"steam_tokenized\")"
   ]
  },
  {
   "source": [
    "## Load Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "# using tokenizer from huggingface\n",
    "tokenizer = BertTokenizerFast.from_pretrained('./steam_tokenized', show_progress=True)\n",
    "tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the two columns we will be using: text(review), label(sentiment)\n",
    "txt_field = data.Field(sequential=True,\n",
    "                       use_vocab=False,\n",
    "                       tokenize=tokenizer.encode,\n",
    "                       include_lengths=True)\n",
    "\n",
    "lbl_field = data.Field(sequential=False,\n",
    "                       use_vocab=False,\n",
    "                       pad_token=None,\n",
    "                       unk_token=None)\n",
    "\n",
    "fields = [\n",
    "    ('review', txt_field),\n",
    "    ('sentiment', lbl_field)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 6min 27s, sys: 1.53 s, total: 6min 29s\nWall time: 6min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# load up the train and validation sets\n",
    "train_ds, val_ds = data.TabularDataset.splits(path='../raw_data', \n",
    "                                              train='train.csv', \n",
    "                                              validation='val.csv', \n",
    "                                              format='csv', \n",
    "                                              fields=fields, \n",
    "                                              skip_header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = data.BucketIterator(train_ds, batch_size=64, sort_key=lambda x: len(x.text), device=gpu, train=True, sort=True, sort_within_batch=True)\n",
    "valid_iter = data.BucketIterator(val_ds, batch_size=64, sort_key=lambda x: len(x.text), device=gpu, train=True, sort=True, sort_within_batch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
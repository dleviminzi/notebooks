''' Data Scrape Script
the information collected (for now) is:
    -> date/time
    -> stock symbol
    -> price movement (percentage)
    -> price
    -> latest article's headline and its preview text

to-do:
    [] !! add dictionary "cache" to store last updates tuples
       only update the tuples in the cache if headline/pct_movement have
       significant changes
    [] if "level 2" data is worth adding (probably is) then add it
    [] add opening price
    [] add average price (short and long term)
    [] ...
'''

from datetime import datetime
from twisted.internet import task, reactor
import robin_stocks as rs
import json
import openai

def scooper(stk, tup_sym):
    '''
    gather/format all of the information listed in primary docstring
    parameters:
        stk     -- dictionary for a stock (generated by get_top_movers)
        tup_sym -- list of tuples with formated information
    '''
    n_title = ''
    n_preview_text = ''

    current_price = rs.get_latest_price(stk['symbol'], includeExtendedHours=True)[0]
    news = rs.stocks.get_news(stk['symbol'])

    try: # to get news
        n_title = news[0]['title']
        n_preview_text = news[0]['preview_text']
        n_preview_text = n_preview_text.replace(',', '')
        n_preview_text = n_preview_text.replace('\n', '')
        n_title = n_title.replace(',', '')
        n_title = n_title.replace('\n', '')
    except IndexError: # that there is no news
        n_title = 'NA'
        n_preview_text = 'NA'

    sentiment = 0
    if n_preview_text != 'NA':
        openai.Completion.create(
            engine="davinci",
            prompt="{}".format(n_preview_text),
            max_tokens=5
        )

    tup_sym.append((stk['symbol'], stk['price_movement']['market_hours_last_movement_pct'], current_price, n_title, n_preview_text))


def collector():
    '''
    pool data from scoopers and write to pooled.csv
    '''

    # open pooled.csv and prepare to append text
    pooled_data = open('./data/pooled.csv', 'a')
    pooled_data.write('\n')

    dtime = datetime.now().strftime('%m/%d/%y,%H:%M')
    print('scraping data... {}'.format(dtime))

    rising_sym = []
    falling_sym = []

    for stk in rs.markets.get_top_movers('up'):
        scooper(stk, rising_sym)

    for stk in rs.markets.get_top_movers('down'):
        scooper(stk, falling_sym)

    pooled_sym = rising_sym + falling_sym

    for sym in pooled_sym:
        pooled_data.write('{},{},{},{},{},{}\n'.format(dtime, sym[0], sym[1], sym[2], sym[3], sym[4]))

    pooled_data.close()
    print('done')